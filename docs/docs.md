# BÃO CÃO Dá»° ÃN: YOUTUBE ANALYTICS PIPELINE

## 1. Tá»”NG QUAN Dá»° ÃN

### 1.1. Giá»›i thiá»‡u
**YouTube Analytics Pipeline** lÃ  má»™t há»‡ thá»‘ng data pipeline hoÃ n chá»‰nh Ä‘á»ƒ thu tháº­p, xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u tá»« YouTube Data API v3. Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn ná»n táº£ng Modern Data Stack vá»›i cÃ¡c cÃ´ng nghá»‡:

- **Data Extraction**: Python + YouTube Data API v3
- **Data Storage**: PostgreSQL (staging) + Google BigQuery (warehouse)
- **Data Transformation**: dbt (data build tool)
- **Orchestration**: Prefect 3.0
- **Visualization**: Streamlit
- **Infrastructure**: Docker Compose

### 1.2. Má»¥c tiÃªu dá»± Ã¡n
- Tá»± Ä‘á»™ng thu tháº­p dá»¯ liá»‡u tá»« nhiá»u kÃªnh YouTube theo lá»‹ch trÃ¬nh
- Xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u thÃ´ thÃ nh cÃ¡c báº£ng phÃ¢n tÃ­ch
- Theo dÃµi hiá»‡u suáº¥t video vÃ  kÃªnh theo thá»i gian
- Tá»‘i Æ°u hÃ³a viá»‡c sá»­ dá»¥ng YouTube API quota (10,000 units/day)
- Cung cáº¥p dashboard trá»±c quan cho viá»‡c ra quyáº¿t Ä‘á»‹nh

### 1.3. Äá»‘i tÆ°á»£ng sá»­ dá»¥ng
- Content creators cáº§n phÃ¢n tÃ­ch nhiá»u kÃªnh YouTube
- Marketing teams theo dÃµi performance cá»§a video campaigns
- Data analysts nghiÃªn cá»©u xu hÆ°á»›ng ná»™i dung YouTube
- Developers muá»‘n tÃ¬m hiá»ƒu vá» modern data pipeline

---

## 2. NGUá»’N Gá»C VÃ€ Káº¾ THá»ªA Tá»ª FIVETRAN

### 2.1. Fivetran YouTube Analytics Connector
Dá»± Ã¡n nÃ y láº¥y cáº£m há»©ng vÃ  káº¿ thá»«a má»™t sá»‘ thÃ nh pháº§n tá»« **Fivetran's dbt YouTube Analytics package**, má»™t open-source package dbt Ä‘á»ƒ transform dá»¯ liá»‡u YouTube tá»« Fivetran connector.

**Fivetran lÃ  gÃ¬?**
- Fivetran lÃ  má»™t ELT (Extract, Load, Transform) platform
- Cung cáº¥p 200+ pre-built connectors Ä‘á»ƒ sync data tá»« cÃ¡c nguá»“n khÃ¡c nhau
- YouTube Analytics connector cá»§a Fivetran tá»± Ä‘á»™ng sync data tá»« YouTube Analytics API
- Dá»¯ liá»‡u Ä‘Æ°á»£c load trá»±c tiáº¿p vÃ o data warehouse (BigQuery, Snowflake, Redshift...)
- Chi phÃ­: $1-2 per monthly active row (MAR)

### 2.2. Nhá»¯ng gÃ¬ Ä‘Æ°á»£c káº¿ thá»«a tá»« Fivetran dbt package

#### a) Cáº¥u trÃºc dbt models theo medallion architecture:
```
staging/       â†’ Clean & standardize raw data
intermediate/  â†’ Business logic transformations  
mart/          â†’ Final analytics tables
```

#### b) Package dependencies trong `packages.yml`:
```yaml
- package: fivetran/fivetran_utils
  version: [">=0.4.0", "<0.5.0"]
- package: dbt-labs/dbt_utils
  version: [">=1.0.0", "<2.0.0"]
```

**fivetran_utils** cung cáº¥p cÃ¡c macros há»¯u Ã­ch:
- `add_pass_through_columns()`: Cho phÃ©p thÃªm custom columns
- `source_relation`: Há»— trá»£ multi-source scenarios
- Testing utilities vÃ  helper functions

#### c) Naming conventions:
- Staging models: `stg_youtube__<entity>`
- Mart models: `fct_` (fact) vÃ  `dim_` (dimension)
- Intermediate models: `int_youtube__<purpose>`

#### d) Integration tests structure:
ThÆ° má»¥c `integration_tests/` vá»›i:
- Integrity tests: Kiá»ƒm tra data consistency
- Consistency tests: So sÃ¡nh vá»›i expected outputs
- Seeds data: Sample data Ä‘á»ƒ test

#### e) Metadata columns:
- `_fivetran_synced`: Timestamp cá»§a láº§n sync cuá»‘i
- `_fivetran_id`: Unique identifier cho má»—i record

### 2.3. Táº¡i sao khÃ´ng dÃ¹ng Fivetran trá»±c tiáº¿p?

**Æ¯u Ä‘iá»ƒm cá»§a Fivetran:**
- âœ… Setup nhanh, khÃ´ng cáº§n code
- âœ… Tá»± Ä‘á»™ng handle errors, retries
- âœ… Managed infrastructure
- âœ… Support nhiá»u data sources

**NhÆ°á»£c Ä‘iá»ƒm vÃ  lÃ½ do build custom solution:**
- âŒ **Chi phÃ­ cao**: $1-2/MAR, vá»›i 15 channels x 50 videos = $750-1500/month
- âŒ **Giá»›i háº¡n kiá»ƒm soÃ¡t**: KhÃ´ng thá»ƒ customize crawl logic
- âŒ **API Quota**: KhÃ´ng control Ä‘Æ°á»£c cÃ¡ch sá»­ dá»¥ng YouTube quota
- âŒ **Vendor lock-in**: Phá»¥ thuá»™c vÃ o Fivetran platform
- âŒ **Learning opportunity**: Máº¥t cÆ¡ há»™i há»c vá» data engineering
- âŒ **Flexibility**: KhÃ³ thÃªm custom features (vÃ­ dá»¥: crawl comments cÃ³ Ä‘iá»u kiá»‡n)

**Lá»£i Ã­ch cá»§a custom solution:**
- âœ… **Miá»…n phÃ­**: Chá»‰ tá»‘n chi phÃ­ GCP/BigQuery
- âœ… **Full control**: TÃ¹y chá»‰nh má»i khÃ­a cáº¡nh cá»§a pipeline
- âœ… **Quota optimization**: Implement smart quota management
- âœ… **Custom features**: ThÃªm báº¥t ká»³ logic nÃ o cáº§n thiáº¿t
- âœ… **Educational**: Há»c vá» data engineering tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i

---

## 3. KIáº¾N TRÃšC Há»† THá»NG

### 3.1. SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YouTube API    â”‚
â”‚   (Source)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Python Crawler
         â”‚ (extract/)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚      â”‚   BigQuery      â”‚
â”‚  (Staging DB)   â”‚â”€â”€â”€â”€â”€â†’â”‚  (Data Warehouse)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                         â”‚
         â”‚                         â”‚ dbt transformation
         â”‚                         â”‚ (dbt_project/)
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                  â†“
    â”‚  Prefect â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚(Scheduler)â”‚           â”‚  Analytics      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  Tables (Mart)  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â†“
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Streamlit     â”‚
                           â”‚   Dashboard     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2. Data Flow chi tiáº¿t

#### Phase 1: Data Extraction (Python)
```python
# extract/crawlers.py
YouTube API â†’ Python Crawler â†’ PostgreSQL (raw JSON)
                              â†’ BigQuery (raw tables)

Tables created:
- raw_videos
- raw_channels  
- raw_playlists
- raw_comments (optional)
```

**Features:**
- Rate limiting: 0.5s delay giá»¯a cÃ¡c requests
- Retry logic: 2 retries vá»›i exponential backoff
- Quota monitoring: Dá»«ng tá»± Ä‘á»™ng khi Ä‘áº¡t 90% quota
- Error handling: Log chi tiáº¿t, khÃ´ng crash toÃ n bá»™ pipeline
- Incremental updates: Chá»‰ crawl videos má»›i hoáº·c cáº§n update

#### Phase 2: Data Transformation (dbt)
```sql
-- dbt_project/models/

Staging Layer (stg_youtube__*):
- Parse JSON thÃ nh columns
- Type casting vÃ  validation
- Deduplicate records
- Add metadata columns

Intermediate Layer (int_youtube__*):
- Join cÃ¡c entities
- Calculate derived metrics
- Apply business logic

Mart Layer (fct_*, dim_*):
- fct_video_performance: Video metrics over time
- dim_channel_summary: Channel aggregations
- agg_daily_metrics: Daily rollups
```

#### Phase 3: Orchestration (Prefect)
```python
# orchestrate/flows/

daily-youtube-analytics:
  Schedule: 2:00 AM daily
  Steps:
    1. Check quota
    2. Crawl channels (limit=15)
    3. Run dbt models
    4. Run dbt tests
    5. Send notifications

extract-3times-daily:
  Schedule: 8:00 AM, 2:00 PM, 8:00 PM
  Steps: Chá»‰ crawl data

dbt-transform-daily:
  Schedule: 30 mins sau extract
  Steps: Transform data má»›i
```

### 3.3. Technology Stack

#### Backend Services:
- **PostgreSQL 14**: Temporary staging storage, metadata
- **Redis 7**: Prefect message queue vÃ  caching
- **Prefect Server 3.0**: Workflow orchestration UI
- **Prefect Worker**: Execute scheduled flows

#### Data Processing:
- **Python 3.12**: Extraction logic
- **dbt-core 1.7.0**: SQL transformations
- **dbt-bigquery**: BigQuery adapter
- **Google BigQuery**: Cloud data warehouse

#### APIs & SDKs:
- **google-api-python-client**: YouTube Data API v3
- **google-cloud-bigquery**: BigQuery Python client
- **psycopg2**: PostgreSQL adapter

#### Development Tools:
- **Docker Compose**: Local development environment
- **Poetry/pip**: Python dependency management
- **Pytest**: Unit testing
- **Ruff**: Python linting

---

## 4. CÃC THÃ€NH PHáº¦N Má»šI (SO Vá»šI FIVETRAN)

### 4.1. Custom Python Crawler (`extract/`)

#### Cáº¥u trÃºc module:
```
extract/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ cli.py           # Command-line interface
â”œâ”€â”€ config.py        # Configuration management
â”œâ”€â”€ crawlers.py      # Core crawling logic
â”œâ”€â”€ db_manager.py    # Database operations
â””â”€â”€ schemas/
    â””â”€â”€ schema_postgres.sql
```

#### TÃ­nh nÄƒng chÃ­nh:

**a) Multi-channel management:**
```python
# config/channels.yml
channels:
  - id: UCXuqSBlHAE6Xw-yeJA0Tunw
    name: Linus Tech Tips
    frequency_hours: 24
    priority: 1
    active: true
    include_comments: false
```

**b) Smart quota management:**
```python
# script/monitor_quota.py
- Real-time quota tracking
- Estimate cost before crawl
- Auto-stop at 90% usage
- Daily quota reset detection
```

**c) CLI tool:**
```bash
# Add channels
python -m extract.cli add <channel_id> "<name>"

# Crawl specific channel
python -m extract.cli crawl --channel <id>

# Crawl from config file
python -m extract.cli crawl-file --limit 10

# View history
python -m extract.cli history

# List all channels
python -m extract.cli channels
```

**d) Incremental crawling:**
- Chá»‰ crawl videos uploaded sau láº§n crawl cuá»‘i
- Update statistics cho videos hiá»‡n cÃ³
- Skip videos Ä‘Ã£ cÃ³ Ä‘áº§y Ä‘á»§ data

**e) Error resilience:**
```python
@retry(max_attempts=3, backoff=exponential)
def fetch_video_data(video_id):
    try:
        # API call
    except HttpError as e:
        if e.resp.status == 403:
            # Quota exceeded
        elif e.resp.status == 404:
            # Video deleted
```

### 4.2. Prefect Orchestration (`orchestrate/`)

#### Cáº¥u trÃºc flows:
```
orchestrate/
â”œâ”€â”€ flows/
â”‚   â”œâ”€â”€ complete_pipeline.py      # Full ETL pipeline
â”‚   â”œâ”€â”€ extract_youtube_data.py   # Data extraction only
â”‚   â””â”€â”€ transform_with_dbt.py     # dbt transformation only
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ youtube_tasks.py          # YouTube API tasks
â”‚   â”œâ”€â”€ dbt_tasks.py             # dbt operations
â”‚   â””â”€â”€ notification_tasks.py     # Alerts & monitoring
â””â”€â”€ deployments/
    â””â”€â”€ deploy_daily_schedule.py
```

#### Prefect advantages vs cron:
- **Visual monitoring**: Web UI Ä‘á»ƒ track runs
- **Retry logic**: Automatic retry vá»›i configurable strategy
- **Alerting**: Email/Slack notifications on failure
- **Parameterization**: Easy to change parameters
- **Concurrency control**: Prevent overlapping runs
- **Logging**: Centralized log storage

### 4.3. Custom dbt Macros

#### a) `parse_iso8601_duration.sql`
```sql
-- Convert PT1H23M45S â†’ 5025 seconds
{{ parse_iso8601_duration('PT1H23M45S') }}
```

YouTube API tráº£ vá» duration theo ISO 8601 format. Macro nÃ y convert sang seconds Ä‘á»ƒ dá»… tÃ­nh toÃ¡n.

#### b) `deduplicate_by_latest.sql`
```sql
-- Keep only latest version of each record
{{ deduplicate_by_latest(
    'raw_videos',
    'id',
    'ingestion_time'
) }}
```

Xá»­ lÃ½ trÆ°á»ng há»£p crawl video nhiá»u láº§n trong ngÃ y, chá»‰ giá»¯ version má»›i nháº¥t.

#### c) `get_layer_schema.sql`
```sql
-- Dynamic schema naming
-- staging â†’ stg_yt
-- intermediate â†’ int_yt
-- mart â†’ mart_yt
```

Tá»• chá»©c schemas theo layer cho dá»… quáº£n lÃ½.

#### d) `get_passthrough_columns.sql`
```sql
-- Allow users to add custom columns
-- without modifying source code
{{ get_passthrough_columns(['custom_tag', 'internal_notes']) }}
```

### 4.4. Data Quality Framework

#### Built-in tests:
```sql
-- tests/assert_positive_statistics.sql
SELECT * FROM {{ ref('fct_video_performance') }}
WHERE view_count < 0
   OR like_count < 0
   OR comment_count < 0

-- tests/assert_valid_engagement_rates.sql  
SELECT * FROM {{ ref('fct_video_performance') }}
WHERE engagement_rate > 1.0
   OR engagement_rate < 0
```

#### dbt tests in models:
```yaml
# models/staging/_stg_youtube__models.yml
columns:
  - name: video_id
    tests:
      - unique
      - not_null
  - name: view_count
    tests:
      - not_null
      - dbt_utils.accepted_range:
          min_value: 0
```

### 4.5. Streamlit Dashboard (`serve/`)

#### Features:
- **Overview metrics**: Total views, subscribers, videos
- **Channel comparison**: Side-by-side performance
- **Video analytics**: Individual video deep-dive
- **Trend analysis**: Time-series charts
- **Category insights**: Performance by video category
- **Export functionality**: Download data as CSV

#### Charts implemented:
- Line charts: Views/likes over time
- Bar charts: Top performing videos
- Scatter plots: Engagement vs views
- Heatmaps: Upload patterns by day/hour
- Pie charts: Video distribution by category

### 4.6. Infrastructure as Code

#### Docker Compose setup:
```yaml
services:
  postgres:       # Metadata & staging
  redis:          # Prefect message queue
  prefect-server: # Orchestration UI
  prefect-worker: # Task execution
  streamlit:      # Dashboard (optional)
```

#### Benefits:
- **Reproducible**: Báº¥t ká»³ ai cÅ©ng cÃ³ thá»ƒ setup giá»‘ng há»‡t
- **Isolated**: KhÃ´ng áº£nh hÆ°á»Ÿng system packages
- **Scalable**: Dá»… dÃ ng scale services riÃªng láº»
- **Version controlled**: Infrastructure changes tracked in Git

### 4.7. Development Tools

#### a) Makefile shortcuts:
```makefile
make setup          # Initialize database
make up            # Start services
make down          # Stop services
make crawl         # Run extraction
make dbt-run       # Run transformations
make prefect-deploy # Deploy workflows
```

#### b) Scripts automation:
```
script/
â”œâ”€â”€ setup_all.py           # One-click setup
â”œâ”€â”€ monitor_quota.py       # Quota monitoring
â”œâ”€â”€ bulk_add_channels.py   # Import channels from CSV
â”œâ”€â”€ deploy_prefect.py      # Automated deployment
â””â”€â”€ dbt_cli.py            # dbt wrapper with logging
```

#### c) Configuration management:
```
config/
â”œâ”€â”€ channels.yml           # Channel definitions
â”œâ”€â”€ channels_template.csv  # Bulk import template
â””â”€â”€ prefect.yaml          # Workflow schedules
```

---

## 5. PHÃ‚N TÃCH SO SÃNH

### 5.1. Fivetran Solution vs Custom Solution

| Aspect | Fivetran | Custom (This Project) |
|--------|----------|----------------------|
| **Setup Time** | 15 minutes | 2-3 hours |
| **Monthly Cost** | $750-1500 | $10-50 (GCP only) |
| **Code Required** | Minimal (dbt only) | Extensive (Python + dbt) |
| **Customization** | Limited | Unlimited |
| **Maintenance** | Low (managed) | Medium (self-managed) |
| **Learning Curve** | Low | High |
| **Quota Control** | No | Yes |
| **Feature Addition** | Depends on Fivetran | Immediate |
| **Scalability** | Automatic | Manual (but flexible) |
| **Data Freshness** | Fixed schedule | Custom schedule |

### 5.2. Khi nÃ o nÃªn dÃ¹ng Fivetran?

âœ… **NÃªn dÃ¹ng Fivetran khi:**
- Budget khÃ´ng lÃ  váº¥n Ä‘á»
- Cáº§n setup nhanh (production ASAP)
- Team khÃ´ng cÃ³ Python developers
- Cáº§n sync nhiá»u data sources (>10)
- Æ¯u tiÃªn stability over customization

### 5.3. Khi nÃ o nÃªn dÃ¹ng Custom Solution?

âœ… **NÃªn dÃ¹ng Custom Solution khi:**
- Budget giá»›i háº¡n hoáº·c startup stage
- Cáº§n control chi tiáº¿t crawling logic
- Muá»‘n optimize API quota usage
- CÃ³ yÃªu cáº§u custom features
- Team cÃ³ Python & data engineering skills
- Muá»‘n há»c vá» modern data engineering

---

## 6. ÄIá»‚M Máº NH Cá»¦A Dá»° ÃN

### 6.1. Technical Excellence

**1. Idempotency:**
```python
# Cháº¡y láº¡i nhiá»u láº§n â†’ cÃ¹ng káº¿t quáº£
# KhÃ´ng táº¡o duplicate records
# Safe Ä‘á»ƒ retry failed runs
```

**2. Incremental Loading:**
```python
# Chá»‰ process data má»›i
# Tiáº¿t kiá»‡m API quota
# Faster execution
```

**3. Type Safety:**
```python
# Pydantic models cho validation
# Type hints everywhere
# Catch errors at parse time
```

**4. Observability:**
- Detailed logging at every step
- Metrics tracking (quota, rows processed)
- Error notifications
- Prefect UI monitoring

### 6.2. Best Practices

**1. Separation of Concerns:**
```
extract/     â†’ Data collection
dbt_project/ â†’ Data transformation
orchestrate/ â†’ Workflow management
serve/       â†’ Data presentation
```

**2. Configuration Management:**
- Environment variables cho secrets
- YAML files cho declarative config
- Template files cho easy setup

**3. Testing:**
- Unit tests cho Python code
- dbt tests cho data quality
- Integration tests vá»›i sample data

**4. Documentation:**
- Inline comments
- README vá»›i examples
- Schema documentation trong dbt

### 6.3. Production-Ready Features

- âœ… **Error handling**: Comprehensive try-catch
- âœ… **Retry logic**: Exponential backoff
- âœ… **Rate limiting**: Respect API limits
- âœ… **Quota monitoring**: Prevent overages
- âœ… **Data validation**: Schema enforcement
- âœ… **Logging**: Structured logs
- âœ… **Alerts**: Failure notifications
- âœ… **Rollback**: Version control vá»›i dbt
- âœ… **Monitoring**: Prefect dashboard
- âœ… **Scalability**: Container-based

---

## 7. Háº NG CHáº¾ VÃ€ CÃCH KHáº®C PHá»¤C

### 7.1. Limitations hiá»‡n táº¡i

**1. API Quota Constraints:**
- âŒ Giá»›i háº¡n 10,000 units/day
- âŒ KhÃ´ng thá»ƒ crawl real-time
- âœ… **Solution**: Smart scheduling, prioritize channels

**2. No Historical Data:**
- âŒ Chá»‰ cÃ³ data tá»« khi báº¯t Ä‘áº§u crawl
- âŒ KhÃ´ng access Ä‘Æ°á»£c YouTube Analytics API
- âœ… **Solution**: Start crawling ASAP, backfill where possible

**3. Single-threaded Crawling:**
- âŒ Sequential API calls
- âŒ Slow khi cÃ³ nhiá»u channels
- âœ… **Solution**: CÃ³ thá»ƒ implement parallel crawling vá»›i semaphore

**4. Local Deployment:**
- âŒ Requires machine running 24/7
- âŒ No cloud deployment yet
- âœ… **Solution**: Deploy lÃªn GCP Cloud Run or AWS ECS

### 7.2. Future Enhancements

**Phase 2 - Cloud Native:**
```
- Deploy Prefect to Cloud Run
- Use Cloud Scheduler instead of local
- Cloud SQL instead of local PostgreSQL
- Terraform for infrastructure
```

**Phase 3 - Advanced Analytics:**
```
- Sentiment analysis on comments
- Thumbnail analysis vá»›i Vision API
- Competitor analysis
- Trend prediction vá»›i ML
```

**Phase 4 - Multi-platform:**
```
- Add TikTok data
- Add Instagram data
- Cross-platform analytics
```

---

## 8. HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG NÃ‚NG CAO

### 8.1. Customize Crawling Logic

```python
# extract/crawlers.py

# ThÃªm custom fields
def enrich_video_data(video):
    video['custom_score'] = calculate_score(video)
    video['trending'] = is_trending(video)
    return video

# Conditional crawling
def should_crawl_comments(video):
    return video['view_count'] > 10000
```

### 8.2. Extend dbt Models

```sql
-- models/mart/custom_metrics.sql

{{ config(
    materialized='incremental',
    unique_key='video_id'
) }}

SELECT
    video_id,
    {{ calculate_virality_score() }} as virality,
    {{ predict_future_views() }} as predicted_views
FROM {{ ref('fct_video_performance') }}
```

### 8.3. Add Custom Dashboards

```python
# serve/pages/custom_analysis.py

import streamlit as st

def show_custom_analysis():
    st.title("Custom Analysis")
    # Your custom logic
```

---

## 9. Káº¾T LUáº¬N

### 9.1. ThÃ nh tá»±u Ä‘áº¡t Ä‘Æ°á»£c

Dá»± Ã¡n **YouTube Analytics Pipeline** Ä‘Ã£ thÃ nh cÃ´ng trong viá»‡c:

1. âœ… XÃ¢y dá»±ng má»™t **end-to-end data pipeline** hoÃ n chá»‰nh
2. âœ… Tiáº¿t kiá»‡m **$750-1500/month** so vá»›i Fivetran
3. âœ… CÃ³ **full control** over data collection vÃ  processing
4. âœ… Implement **production-grade features** (retry, monitoring, testing)
5. âœ… Táº¡o **learning resource** cho data engineering community
6. âœ… Ãp dá»¥ng **modern data stack** best practices
7. âœ… Káº¿ thá»«a **proven patterns** tá»« Fivetran dbt package

### 9.2. BÃ i há»c kinh nghiá»‡m

**Technical Lessons:**
- dbt lÃ  cÃ´ng cá»¥ máº¡nh máº½ cho data transformation
- Prefect giÃºp orchestration dá»… dÃ ng hÆ¡n nhiá»u so vá»›i Airflow
- API quota management lÃ  critical cho YouTube projects
- Docker Compose makes development reproducible

**Business Lessons:**
- Build vs Buy decision phá»¥ thuá»™c vÃ o context
- Open-source packages tiáº¿t kiá»‡m ráº¥t nhiá»u effort
- Documentation lÃ  investment, not cost
- Automation saves time in the long run

### 9.3. Recommendations

**Cho beginners:**
- Start vá»›i Fivetran Ä‘á»ƒ hiá»ƒu data flow
- Sau Ä‘Ã³ build custom Ä‘á»ƒ hiá»ƒu deeper
- Focus vÃ o data modeling (dbt) trÆ°á»›c
- Orchestration cÃ³ thá»ƒ dÃ¹ng cron trÆ°á»›c rá»“i migrate sang Prefect

**Cho advanced users:**
- Extend project vá»›i ML models
- Implement real-time streaming vá»›i Pub/Sub
- Add more data sources
- Build recommendation engine

### 9.4. ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng

Dá»± Ã¡n nÃ y lÃ  má»™t **excellent learning project** vÃ  **viable production solution** cho:
- Small to medium YouTube creators (5-20 channels)
- Marketing agencies managing client channels
- Data analysts muá»‘n portfolio project
- Startups cáº§n cost-effective analytics

**ROI (Return on Investment):**
- Development time: 40-60 hours
- Monthly savings: $750-1500
- Payback period: < 1 month
- Learning value: Priceless ğŸš€

---

## PHá»¤ Lá»¤C

### A. Tech Stack Chi Tiáº¿t

#### A.1. Python Dependencies
```toml
# Core
python = "^3.12"
google-api-python-client = "^2.100.0"
google-cloud-bigquery = "^3.23.1"
psycopg2-binary = "^2.9.9"

# Orchestration
prefect = "^3.0.0"
prefect-gcp = "^0.4.0"

# Transformation
dbt-core = "^1.7.0"
dbt-bigquery = "^1.7.0"

# Analytics
pandas = "^2.2.0"
numpy = "^1.26.0"

# Dashboard
streamlit = "^1.32.0"
plotly = "^5.20.0"
```

#### A.2. dbt Packages
```yaml
packages:
  - package: fivetran/fivetran_utils
    version: [">=0.4.0", "<0.5.0"]
  - package: dbt-labs/dbt_utils
    version: [">=1.0.0", "<2.0.0"]
  - package: dbt-labs/spark_utils
    version: [">=0.3.0", "<0.4.0"]
```

### B. Database Schemas

#### B.1. PostgreSQL (Staging)
```sql
-- Channels metadata
CREATE TABLE channels (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    frequency_hours INTEGER DEFAULT 24,
    priority INTEGER DEFAULT 1,
    active BOOLEAN DEFAULT TRUE,
    include_comments BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Crawl history
CREATE TABLE crawl_history (
    id SERIAL PRIMARY KEY,
    channel_id TEXT REFERENCES channels(id),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    status TEXT,
    videos_crawled INTEGER,
    quota_used INTEGER,
    error_message TEXT
);
```

#### B.2. BigQuery (Warehouse)
```sql
-- Raw tables
raw_yt.raw_videos
raw_yt.raw_channels
raw_yt.raw_playlists
raw_yt.raw_comments

-- Staging tables
stg_yt.stg_youtube__videos
stg_yt.stg_youtube__channels
stg_yt.stg_youtube__playlists

-- Mart tables
mart_yt.fct_video_performance
mart_yt.dim_channel_summary
mart_yt.agg_daily_metrics
```

### C. API Quota Breakdown

| Operation | Cost (units) | Notes |
|-----------|--------------|-------|
| channels.list | 1 | Basic channel info |
| playlistItems.list | 1 | Per 50 videos |
| videos.list | 1 | Per 50 videos |
| commentThreads.list | 1 | Per 100 comments |
| search.list | 100 | âŒ Very expensive! |

**Example calculation for 1 channel:**
```
1 channel info          = 1 unit
1 playlist fetch        = 1 unit
50 videos details       = 1 unit
10 videos x comments    = 10 units
Total per channel       = 13 units

Max channels per day    = 10,000 / 13 â‰ˆ 769 channels
Realistic (with buffer) = 15-20 channels
```

### D. Useful Resources

#### Documentation:
- YouTube Data API: https://developers.google.com/youtube/v3
- dbt Docs: https://docs.getdbt.com
- Prefect Docs: https://docs.prefect.io
- BigQuery Docs: https://cloud.google.com/bigquery/docs

#### Related Projects:
- Fivetran dbt YouTube: https://github.com/fivetran/dbt_youtube_analytics
- Meltano (Open-source ELT): https://meltano.com
- Singer Taps: https://www.singer.io

---

**Document Version**: 1.0  
**Last Updated**: February 23, 2026  
**Author**: YouTube Analytics Pipeline Team  
**License**: MIT
